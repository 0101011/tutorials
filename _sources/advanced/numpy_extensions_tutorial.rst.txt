

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating extensions using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of it’s implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of it’s implementation



.. code-block:: python


    import torch
    from torch.autograd import Function







Parameter-less example
----------------------

This layer doesn’t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**



.. code-block:: python


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):

        def forward(self, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        def backward(self, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction()(input)







**Example usage of the created layer:**



.. code-block:: python


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 Out::

    tensor([[  4.7799,   5.0959,   5.6019,   6.9106,   1.0116],
            [  7.6462,   9.6253,   6.1617,   4.4801,   5.0210],
            [  8.8374,   6.0468,  10.7504,   7.2650,   4.0067],
            [  5.5755,  13.4731,   7.5455,   3.8536,   2.3332],
            [  3.0095,  18.1805,  10.1975,   4.0043,  11.2561],
            [  5.5755,   5.0320,   6.2547,   8.6963,   2.3332],
            [  8.8374,  10.8438,   8.6336,  13.1542,   4.0067],
            [  7.6462,   2.4751,   7.5721,   7.6009,   5.0210]])
    tensor([[ 2.4388,  0.8758, -0.4533,  1.2186, -0.5066, -0.8348, -0.7643,
              0.3721],
            [ 0.6125, -1.0402, -0.1780, -0.4918,  0.4554, -0.3364,  2.3092,
              2.3362],
            [ 0.8486, -1.2320, -0.2627,  2.0324,  0.8668,  0.3895, -1.7898,
             -0.0513],
            [-0.0660,  0.1421, -1.4583, -0.6590, -1.2496,  1.2484,  1.6170,
             -0.6128],
            [-1.5694, -0.0826, -1.2402, -0.5830,  0.4311,  0.2312, -0.6741,
             -0.5953],
            [ 0.4418,  0.0818,  0.7729, -0.9260,  0.6533, -0.6825,  0.3457,
              1.4377],
            [ 0.0027,  0.1899,  0.9237,  1.8662, -0.1613,  0.0232, -0.2083,
             -0.8160],
            [ 0.4953, -1.4919,  0.0697, -0.3827, -0.0543,  1.7160,  0.2477,
             -1.4584]])


Parametrized example
--------------------

This implements a layer with learnable weights.

It implements the Cross-correlation with a learnable kernel.

In deep learning literature, it’s confusingly referred to as
Convolution.

The backward computes the gradients wrt the input and gradients wrt the
filter.

**Implementation:**

*Please Note that the implementation serves as an illustration, and we
did not verify it’s correctness*



.. code-block:: python


    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter):
            input, filter = input.detach(), filter.detach()  # detach so we can cast to NumPy
            result = correlate2d(input.numpy(), filter.detach().numpy(), mode='valid')
            ctx.save_for_backward(input, filter)
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter = ctx.saved_tensors
            grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full')
            grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid')

            return grad_output.new_tensor(grad_input), grad_output.new_tensor(grad_filter)


    class ScipyConv2d(Module):

        def __init__(self, kh, kw):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(kh, kw))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter)







**Example usage:**



.. code-block:: python


    module = ScipyConv2d(3, 3)
    print(list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print(output)
    output.backward(torch.randn(8, 8))
    print(input.grad)




.. rst-class:: sphx-glr-script-out

 Out::

    [Parameter containing:
    tensor([[-0.8526,  1.2830, -1.0671],
            [-0.7071,  0.8099,  0.7893],
            [ 0.7218, -0.5466,  0.7299]])]
    tensor([[ 2.9631, -2.1939,  3.7564, -0.1147, -0.7655, -0.3945, -1.3504,
             -1.9588],
            [ 4.6629, -2.4375,  0.5827,  0.1887, -2.2122,  2.7010, -0.1724,
              0.4267],
            [ 1.3938,  0.8749,  4.0858, -0.2377, -0.5029, -1.1324,  5.4541,
             -3.8162],
            [-1.3109,  3.2396, -2.0251,  2.0249, -0.9814, -6.1262,  1.7355,
              2.4721],
            [-4.0241,  1.5372, -0.9288, -0.8160,  4.6906, -5.4825, -1.1887,
              0.2399],
            [-1.0353, -3.3026,  1.8125, -1.3914, -1.8146,  4.8768, -5.4724,
             -1.2333],
            [-0.7533, -1.4271, -2.5114,  1.5470, -1.9201,  1.8501,  0.6626,
              0.8288],
            [ 0.4698,  0.0476, -0.0547, -1.6718,  3.5252, -0.2415,  3.0243,
              0.5828]])
    tensor([[-0.9851, -1.0390,  0.5850, -0.1611, -0.4086,  0.6041,  1.2194,
              0.9298,  0.2422, -0.7570],
            [ 1.9712,  1.9514,  0.0722,  0.6515, -0.6652,  0.0590,  0.2476,
             -1.6474,  0.7402, -0.5444],
            [-1.7172,  0.2778,  0.1028,  0.0973,  3.9202, -0.4434, -1.9013,
              0.9384, -3.0599, -0.3769],
            [-0.6669, -0.9977,  2.4371, -1.6561, -5.2731,  3.6980, -0.6422,
             -0.6934, -0.5314, -1.6206],
            [ 0.9558, -0.1515, -3.0855,  0.7897,  1.5289, -4.1926,  0.3678,
             -2.0602, -0.8272, -0.5292],
            [ 1.3532,  2.1806, -0.1146, -1.2690,  1.7193,  0.1185, -1.3095,
              3.4624, -3.2794, -1.0922],
            [-2.5025,  2.5688,  2.4851,  0.6182, -4.7862, -2.7110, -3.2023,
             -2.7244,  0.6007,  0.0368],
            [ 0.7026, -4.5458, -2.1221, -0.9031,  4.7567,  3.7623,  4.8492,
             -1.9101, -3.6822, -0.4252],
            [ 1.0778,  0.8104, -0.5642,  0.0618, -5.2415, -5.2402, -0.9400,
              3.5293,  1.7094, -0.2035],
            [-0.3688,  0.7808, -0.9192,  0.8505,  0.9748,  1.7210, -2.0611,
             -3.0633,  0.1092,  0.4839]])


**Total running time of the script:** ( 0 minutes  0.006 seconds)



.. only :: html

 .. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
