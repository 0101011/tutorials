{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python"
    }
  },
  "cells": [
    {
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "\nAutograd\n========\n\nAutograd is now a core torch package for automatic differentiation.\nIt uses a tape based system for automatic differentiation.\n\nIn the forward phase, the autograd tape will remember all the operations\nit executed, and in the backward phase, it will replay the operations.\n\nVariable\n--------\n\nIn autograd, we introduce a ``Variable`` class, which is a very thin\nwrapper around a ``Tensor``. You can access the raw tensor through the\n``.data`` attribute, and after computing the backward pass, a gradient\nw.r.t. this variable is accumulated into ``.grad`` attribute.\n\n.. figure:: /_static/img/Variable.png\n   :alt: Variable\n\n   Variable\n\nThere\u2019s one more class which is very important for autograd\nimplementation - a ``Function``. ``Variable`` and ``Function`` are\ninterconnected and build up an acyclic graph, that encodes a complete\nhistory of computation. Each variable has a ``.grad_fn`` attribute that\nreferences a function that has created a function (except for Variables\ncreated by the user - these have ``None`` as ``.grad_fn``).\n\nIf you want to compute the derivatives, you can call ``.backward()`` on\na ``Variable``. If ``Variable`` is a scalar (i.e. it holds a one element\ntensor), you don\u2019t need to specify any arguments to ``backward()``,\nhowever if it has more elements, you need to specify a ``grad_output``\nargument that is a tensor of matching shape.\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "import torch\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad=True)\nprint(x)  # notice the \"Variable containing\" line"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "execution_count": null,
      "source": [
        "print(x.data)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "execution_count": null,
      "source": [
        "print(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "execution_count": null,
      "source": [
        "print(x.grad_fn)  # we've created x ourselves"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "Do an operation of x:\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "y = x + 2\nprint(y)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "y was created as a result of an operation,\nso it has a grad_fn\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "print(y.grad_fn)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "More operations on y:\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "z = y * y * 3\nout = z.mean()\n\nprint(z, out)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "Gradients\n---------\n\nlet's backprop now and print gradients d(out)/dx\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "out.backward()\nprint(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "By default, gradient computation flushes all the internal buffers\ncontained in the graph, so if you even want to do the backward on some\npart of the graph twice, you need to pass in ``retain_variables = True``\nduring the first pass.\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "x = Variable(torch.ones(2, 2), requires_grad=True)\ny = x + 2\ny.backward(torch.ones(2, 2), retain_graph=True)\n# the retain_variables flag will prevent the internal buffers from being freed\nprint(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "execution_count": null,
      "source": [
        "z = y * y\nprint(z)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    },
    {
      "source": [
        "just backprop random gradients\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "gradient = torch.randn(2, 2)\n\n# this would fail if we didn't specify\n# that we want to retain variables\ny.backward(gradient)\n\nprint(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code"
    }
  ]
}