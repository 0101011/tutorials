

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating extensions using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of it’s implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of it’s implementation



.. code-block:: python


    import torch
    from torch.autograd import Function
    from torch.autograd import Variable







Parameter-less example
----------------------

This layer doesn’t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**



.. code-block:: python


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):

        def forward(self, input):
            numpy_input = input.numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        def backward(self, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction()(input)







**Example usage of the created layer:**



.. code-block:: python


    input = Variable(torch.randn(8, 8), requires_grad=True)
    result = incorrect_fft(input)
    print(result.data)
    result.backward(torch.randn(result.size()))
    print(input.grad)





.. rst-class:: sphx-glr-script-out

 Out::

    6.9459   3.4581   4.6766   0.4686   6.5389
     10.8245   3.7885   1.6736   8.6222   6.7211
      5.1874   6.7855   7.4119   9.5140  12.4209
      6.6348  10.1983   2.3288  10.7623   7.5794
      3.6570   2.2497   8.3797  10.1616   4.3526
      6.6348   4.1459   6.9720  23.9565   7.5794
      5.1874   9.6118   9.5390   8.1721  12.4209
     10.8245   9.4980   2.0711   5.0849   6.7211
    [torch.FloatTensor of size 8x5]

    Variable containing:
     0.1981  0.1934 -0.1629  0.0793 -0.0047  0.0793 -0.1629  0.1934
    -0.0621 -0.0079  0.0194  0.2352 -0.0491 -0.0581  0.1086 -0.3258
    -0.0547 -0.0460  0.0610 -0.1543 -0.1811  0.0359  0.1161  0.1369
    -0.3287  0.1030  0.1182  0.0021 -0.0509 -0.1169 -0.1005  0.0815
    -0.0509 -0.0689 -0.0560  0.0668 -0.2230  0.0668 -0.0560 -0.0689
    -0.3287  0.0815 -0.1005 -0.1169 -0.0509  0.0021  0.1182  0.1030
    -0.0547  0.1369  0.1161  0.0359 -0.1811 -0.1543  0.0610 -0.0460
    -0.0621 -0.3258  0.1086 -0.0581 -0.0491  0.2352  0.0194 -0.0079
    [torch.FloatTensor of size 8x8]


Parametrized example
--------------------

This implements a layer with learnable weights.

It implements the Cross-correlation with a learnable kernel.

In deep learning literature, it’s confusingly referred to as
Convolution.

The backward computes the gradients wrt the input and gradients wrt the
filter.

**Implementation:**

*Please Note that the implementation serves as an illustration, and we
did not verify it’s correctness*



.. code-block:: python


    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter):
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            ctx.save_for_backward(input, filter)
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            input, filter = ctx.saved_tensors
            grad_output = grad_output.data
            grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full')
            grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid')

            return Variable(grad_output.new(grad_input)), \
                Variable(grad_output.new(grad_filter))


    class ScipyConv2d(Module):

        def __init__(self, kh, kw):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(kh, kw))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter)







**Example usage:**



.. code-block:: python


    module = ScipyConv2d(3, 3)
    print(list(module.parameters()))
    input = Variable(torch.randn(10, 10), requires_grad=True)
    output = module(input)
    print(output)
    output.backward(torch.randn(8, 8))
    print(input.grad)




.. rst-class:: sphx-glr-script-out

 Out::

    [Parameter containing:
    -0.6564 -0.3029 -1.0349
    -0.9106 -0.4218  2.2911
     0.4136 -1.0284  0.0007
    [torch.FloatTensor of size 3x3]
    ]
    Variable containing:
     1.4764 -0.3741 -1.1287 -3.1632  1.0703 -0.7127  1.1286  1.4903
     0.6379  4.5509  0.0620  1.3448  0.6657 -0.3458 -1.0090 -2.8121
    -4.1423  0.5284 -2.6284 -5.1020 -2.4626  4.3569  0.2454 -1.5257
     2.6980 -0.2542 -0.3166  4.2060  0.0156  0.7949  4.0229 -3.9827
    -1.6887  0.3985 -1.3981 -4.2095  0.6883 -1.8512  1.1311  2.6494
     0.3621  0.0349  2.1463 -0.3721  2.0332  2.8129  2.2638  0.9475
     3.2486  2.3941 -3.7429 -0.0190 -0.9870 -1.5489 -1.2908 -0.6381
     3.3481 -3.1060 -6.6821 -2.7916  2.6294 -5.3366  2.5514  3.4600
    [torch.FloatTensor of size 8x8]

    Variable containing:
    -0.1016 -0.9003  0.1452  1.8145 -0.9783 -0.7459 -0.0174  1.2910 -2.6348  0.7976
    -0.7341 -0.5658  1.4868 -0.8583  1.4165 -1.1700 -0.3888 -2.5315  2.4590 -2.1760
    -0.9342 -2.0429  4.8819 -0.5701  0.0578 -1.9886  6.2657 -4.7217  3.7186 -0.1621
    -1.1707  2.9564 -3.0950  1.3047  1.1907  2.4033 -1.4147  6.4871  0.1271  1.8404
    -0.9694  0.5291  4.3857 -4.4992  2.4832  3.9441  1.7698 -1.1396 -1.3193 -1.0876
     0.6434 -0.3472  0.9116 -2.2157  0.9210  2.0308  3.7293 -2.3394  1.1593  1.4100
    -1.6169  0.7466  1.5074  0.5020  0.6296 -2.5518  0.0207  2.0205 -4.1593 -0.0798
     0.3138 -4.0341 -5.2514  0.6812  2.3160 -2.7475  1.6144 -0.1323  1.0135 -0.4099
    -2.2022  5.7402 -5.0497 -2.3505 -1.0957 -3.8781 -1.4619 -8.0846  2.1727 -1.6959
     0.0518 -2.3562  2.8809  4.3172 -0.2523 -0.2604  5.4354 -2.6768  3.7796  0.0011
    [torch.FloatTensor of size 10x10]


**Total running time of the script:** ( 0 minutes  0.002 seconds)



.. only :: html

 .. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
