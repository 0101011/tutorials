{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch for former Torchies\n",
    "\n",
    "In this tutorial, you will learn the following:\n",
    "\n",
    "1. Using torch Tensors, and important difference against (Lua)Torch\n",
    "2. Using the autograd package\n",
    "3. Building neural networks\n",
    "  - Building a ConvNet\n",
    "  - Building a Recurrent Net\n",
    "  - Using multiple GPUs\n",
    "\n",
    "\n",
    "## Tensors \n",
    "\n",
    "Tensors behave almost exactly the same way in PyTorch as they do in Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(10, 20)\n",
    "# creates tensor of size (10 x 20) with uninitialized memory\n",
    "\n",
    "a = torch.randn(10, 20)\n",
    "# initializes a tensor randomized with a normal distribution with mean=0, var=1\n",
    "\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inplace / Out-of-place\n",
    "\n",
    "The first difference is that ALL operations on the tensor that operate in-place on it will have an `_` postfix.\n",
    "For example, `add` is the out-of-place version, and `add_` is the in-place version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations like narrow do not have in-place versions, and hence, .narrow_ does not exist. \n",
    "Similarly, some operations like fill_ do not have an out-of-place version, so .fill does not exist.\n",
    "\n",
    "### Zero Indexing\n",
    "\n",
    "Another difference is that Tensors are zero-indexed. (Torch tensors are one-indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a[0,3] # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be also indexed with Python's slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a[:,3:5] # selects all rows, columns 3 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No camel casing\n",
    "\n",
    "The next small difference is that all functions are now NOT camelCase anymore.\n",
    "For example `indexAdd` is now called `index_add_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "  10  100\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor(5, 2)\n",
    "z[:,0] = 10\n",
    "z[:,1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      " 101    1    1    1   11\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.LongTensor([4,0]), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Bridge\n",
    "\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.\n",
    "\n",
    "#### Converting torch Tensor to numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) # see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting numpy Array to torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b) # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to NumPy and back.\n",
    "\n",
    "### CUDA Tensors\n",
    "\n",
    "CUDA Tensors are nice and easy in pytorch, and they are much more consistent as well.\n",
    "Transfering a CUDA tensor from the CPU to GPU will retain it's type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates a LongTensor and transfers it \n",
    "# to GPU as torch.cuda.LongTensor\n",
    "a = torch.LongTensor(10).fill_(3).cuda()\n",
    "print(type(a))\n",
    "b = a.cpu()\n",
    "# transfers it to CPU, back to \n",
    "# being a torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd is now a core torch package for automatic differentiation. \n",
    "\n",
    "It uses a tape based system for automatic differentiation. \n",
    "\n",
    "In the forward phase, the autograd tape will remember all the operations it executed, and in the backward phase, it will replay the operations.\n",
    "\n",
    "In autograd, we introduce a `Variable` class, which is a very thin wrapper around a `Tensor`. \n",
    "You can access the raw tensor through the `.data` attribute, and after computing the backward pass, a gradient w.r.t. this variable is accumulated into `.grad` attribute.\n",
    "\n",
    "![Variable](images/Variable.png)\n",
    "\n",
    "There's one more class which is very important for autograd implementation - a `Function`. `Variable` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each variable has a `.creator` attribute that references a function that has created a function (except for Variables created by the user - these have `None` as  `.creator`).\n",
    "\n",
    "If you want to compute the derivatives, you can call `.backward()` on a `Variable`. \n",
    "If `Variable` is a scalar (i.e. it holds a one element tensor), you don't need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `grad_output` argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "x  # notice the \"Variable containing\" line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.creator is None  # we've created x ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd._functions.basic_ops.AddConstant at 0x107d22690>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.creator\n",
    "# y was created as a result of an operation, \n",
    "# so it has a creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 27  27\n",
       " 27  27\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 27\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's backprop now\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 4.5000  4.5000\n",
       " 4.5000  4.5000\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print gradients d(out)/dx\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, gradient computation flushes all the internal buffers contained in the graph, so if you even want to do the backward on some part of the graph twice, you need to pass in `retain_variables = True` during the first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "y = x + 2\n",
    "y.backward(torch.ones(2, 2), retain_variables=True)\n",
    "# the retain_variables flag will prevent the internal buffers from being freed\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 9  9\n",
       " 9  9\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " -8.4569  14.8427\n",
       " -1.6838   2.6552\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = torch.randn(2, 2)\n",
    "# just backproping random gradients\n",
    "\n",
    "z.backward(gradient)\n",
    "\n",
    "# this would fail if we didn't specify \n",
    "# that we want to retain variables\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've redesigned the nn package, so that it's much more intuitive and fully integrated with autograd.\n",
    "\n",
    "### Replace containers with autograd\n",
    "\n",
    "You no longer have to use Containers like ConcatTable, or modules like CAddTable, or use and debug with nngraph. \n",
    "We will seamlessly use autograd to define our neural networks.\n",
    "For example, \n",
    "\n",
    "`output = CAddTable():forward({input1, input2})` simply becomes `output = input1 + input2`\n",
    "\n",
    "`output = MulConstant(0.5):forward(input)` simply becomes `output = input * 0.5`\n",
    "\n",
    "### State is no longer held in the module, but in the network graph\n",
    "\n",
    "Using recurrent networks is a breeze. If you want to create a recurrent network, simply use the same Linear layer multiple times, and the weights are shared.\n",
    "\n",
    "![torch-nn-vs-pytorch-nn](images/torch-nn-vs-pytorch-nn.png)\n",
    "\n",
    "### Simplified debugging\n",
    "\n",
    "Debugging is intuitive using Python's pdb debugger, and **the debugger and stack traces stop at exactly where an error occurred.** What you see is what you get.\n",
    "\n",
    "### Example 1: ConvNet\n",
    "\n",
    "Creating networks is simple. All of your networks are derived from the base class nn.Container.\n",
    "\n",
    "- In the constructor, you declare all the layers you want to use.\n",
    "- In the forward function, you define how your model is going to be run, from input to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MNISTConvNet(nn.Container):\n",
    "    def __init__(self):\n",
    "        # this is the place where you instantiate all your modules\n",
    "        # you can later access them using the same names you've given them in here\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1   = nn.Linear(320, 50)\n",
    "        self.fc2   = nn.Linear(50, 10)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    # it's the forward function that defines the network structure\n",
    "    # we're accepting only a single input in here, but if you want,\n",
    "    # feel free to use more\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(self.relu(self.conv1(input)))\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "\n",
    "        # in your model definition you can go full crazy and use arbitrary\n",
    "        # python code to define your model structure\n",
    "        # all these are perfectly legal, and will be handled correctly \n",
    "        # by autograd:\n",
    "        # if x.gt(0) > x.numel() / 2:\n",
    "        #      ...\n",
    "        # \n",
    "        # you can even do a loop and reuse the same module inside it\n",
    "        # modules no longer hold ephemeral state, so you can use them\n",
    "        # multiple times during your forward pass\n",
    "        # e.g. in this example we're using relu multiple times (but\n",
    "        # modules with parameters will have correct gradients w.r.t.\n",
    "        # their weights as well)\n",
    "        # while x.norm(2) < 10:\n",
    "        #    x = self.conv1(x) \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this ConvNet now is intuitive. \n",
    "You create an instance of it, and you can run things through it and backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (320 -> 50)\n",
      "  (fc2): Linear (50 -> 10)\n",
      "  (relu): ReLU ()\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MNISTConvNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# only mini-batches are supported in all of nn\n",
    "input = Variable(torch.randn(1, 1, 28, 28))\n",
    "out = net(input)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.3598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = Variable(torch.LongTensor([3]))\n",
    "loss_fn = nn.NLLLoss()\n",
    "err = loss_fn(out, target)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's easy to access individual layer weights and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74122408944\n",
      "0.144665827046\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.data.norm()) # norm of the weight\n",
    "print(net.conv1.weight.grad.norm()) # norm of the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Function Hooks\n",
    "We've inspected the weights and the gradients. \n",
    "But how about inspecting / modifying the output and grad_output of a layer?\n",
    "\n",
    "We introduce **hooks** for this purpose.\n",
    "\n",
    "You can register a function on a *Module* or a *Variable*.  \n",
    "The hook can be a forward hook or a backward hook.  \n",
    "The forward hook will be executed when a forward call is executed.  \n",
    "The backward hook will be executed in the backward phase.  \n",
    "Let's look at an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "('input: ', <type 'tuple'>)\n",
      "('input[0]: ', <class 'torch.autograd.variable.Variable'>)\n",
      "('output: ', <class 'torch.autograd.variable.Variable'>)\n",
      "\n",
      "('input size:', torch.Size([1, 10, 12, 12]))\n",
      "('output size:', torch.Size([1, 20, 8, 8]))\n",
      "('output norm:', 17.128590140129425)\n"
     ]
    }
   ],
   "source": [
    "# We register a forward hook on conv2 and print some information\n",
    "def printnorm(self, input, output):\n",
    "    # input is a tuple of packed inputs\n",
    "    # output is a Variable. output.data is the Tensor we are interested\n",
    "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "    print('')\n",
    "    print('input: ', type(input))\n",
    "    print('input[0]: ', type(input[0]))\n",
    "    print('output: ', type(output))\n",
    "    print('')\n",
    "    print('input size:', input[0].size())\n",
    "    print('output size:', output.data.size())\n",
    "    print('output norm:', output.data.norm())\n",
    "\n",
    "net.conv2.register_forward_hook('printnorm', printnorm)\n",
    "\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "('input: ', <type 'tuple'>)\n",
      "('input[0]: ', <class 'torch.autograd.variable.Variable'>)\n",
      "('output: ', <class 'torch.autograd.variable.Variable'>)\n",
      "\n",
      "('input size:', torch.Size([1, 10, 12, 12]))\n",
      "('output size:', torch.Size([1, 20, 8, 8]))\n",
      "('output norm:', 17.128590140129425)\n",
      "Inside Conv2d backward\n",
      "Inside class:Conv2d\n",
      "\n",
      "('grad_input: ', <type 'tuple'>)\n",
      "('grad_input[0]: ', <class 'torch.FloatTensor'>)\n",
      "('grad_output: ', <type 'tuple'>)\n",
      "('grad_output[0]: ', <class 'torch.FloatTensor'>)\n",
      "\n",
      "('grad_input size:', torch.Size([1, 10, 12, 12]))\n",
      "('grad_output size:', torch.Size([1, 20, 8, 8]))\n",
      "('grad_input norm:', 0.02814446826310051)\n"
     ]
    }
   ],
   "source": [
    "# We register a backward hook on conv2 and print some information\n",
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')    \n",
    "    print('Inside class:' + self.__class__.__name__)\n",
    "    print('')    \n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')    \n",
    "    print('grad_input size:', grad_input[0].size())\n",
    "    print('grad_output size:', grad_output[0].size())\n",
    "    print('grad_input norm:', grad_input[0].norm())\n",
    "\n",
    "net.conv2.register_backward_hook('printstats', printgradnorm)\n",
    "\n",
    "out = net(input)\n",
    "err = loss_fn(out, target)\n",
    "err.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full and working MNIST example is located here\n",
    "https://github.com/pytorch/examples/tree/master/mnist\n",
    "\n",
    "### Example 2: Recurrent Net\n",
    "\n",
    "Building recurrent nets with PyTorch is quite a breeze.\n",
    "Since the state of the network is held in the graph and not\n",
    "in the layers, you can simply create an nn.Linear and \n",
    "reuse it over and over again for the recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Container):\n",
    "\n",
    "    # you can also accept arguments in your model constructor\n",
    "    # if you want to parametrize it somehow\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = data_size + hidden_size\n",
    "        super(RNN, self).__init__(\n",
    "            i2h=nn.Linear(input_size, hidden_size),\n",
    "            i2o=nn.Linear(input_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.i2o(input)\n",
    "        return hidden, output\n",
    "\n",
    "rnn = RNN(50, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 10\n",
    "TIMESTEPS = 5\n",
    "batch = Variable(torch.randn(batch_size, 50))\n",
    "hidden = Variable(torch.zeros(batch_size, 20))\n",
    "target = Variable(torch.zeros(batch_size, 10))\n",
    "loss = 0\n",
    "for t in range(TIMESTEPS):                  \n",
    "    # yes! you can reuse the same network several times,\n",
    "    # sum up the losses, and call backward!\n",
    "    hidden, output = rnn(batch, hidden)\n",
    "    loss += loss_fn(output, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complete Language Modeling example using LSTMs and Penn Tree-bank is located here: https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "PyTorch by default has seamless CuDNN integration for ConvNets and Recurrent Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU examples\n",
    "\n",
    "Using Multiple GPUs in PyTorch is seamless, transparent and easy to use.\n",
    "\n",
    "Data Parallelism is implemented using nn.parallel.data_parallel.\n",
    "One can simply use this as a function in your `forward` and it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataParallelModel(nn.Container):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            block1=nn.Linear(10, 20),\n",
    "            block2=nn.Linear(20, 20),\n",
    "            block3=nn.Linear(20, 20),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = nn.parallel.data_parallel(self.block2, x, (0, 1, 2, 3))\n",
    "        x = self.block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code does not need to be changed in CPU-mode.  \n",
    "If in CPU-mode, the tuple of GPU-ids can be (-1,) and data_parallel becomes a no-op, and the line will equivalently become `x = self.block2(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parallel\n",
    "\n",
    "Doing model parallelism is equally simple. Here's a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DistributedModel(nn.Container):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            embedding=nn.Embedding(1000, 10),\n",
    "            rnn=nn.Linear(10, 10).cuda(0),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.cuda(0)\n",
    "        x = self.rnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primitives on which data parallel is implemented upon\n",
    "In general, pytorch's nn.parallel primitives can be used independently.\n",
    "We have implemented simple MPI-like primitives:\n",
    "- replicate: replicate a Module on multiple devices\n",
    "- scatter: distribute the input in the first-dimension\n",
    "- gather: gather and concatenate the input in the first-dimension\n",
    "- parallel_apply: apply a set of already-distributed inputs to a set of already-distributed models.\n",
    "\n",
    "To give a better clarity, here is the implementation of data_parallel using these collectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_parallel(module, input, device_ids, output_device=None):\n",
    "    \"\"\"Distributes replicas of module accross gpus given in device_ids,\n",
    "       slices the input and applies the copies in parallel.\n",
    "       Outputs are concatenated on the same device as input (or on\n",
    "       output_device if specified). Device id -1 means the CPU.\n",
    "    \"\"\"\n",
    "    if not device_ids:\n",
    "        return module(input)\n",
    "    replicas = replicate(module, device_ids)\n",
    "    inputs = scatter(input, device_ids)\n",
    "    outputs = parallel_apply(replicas, inputs)\n",
    "    if output_device is None:\n",
    "        output_device = -1 if not input.is_cuda else input.get_device()\n",
    "    return gather(outputs, output_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
