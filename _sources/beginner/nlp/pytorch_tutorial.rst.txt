

.. _sphx_glr_beginner_nlp_pytorch_tutorial.py:


Introduction to PyTorch
***********************

Introduction to Torch's tensor library
======================================

All of deep learning is computations on tensors, which are
generalizations of a matrix that can be indexed in more than 2
dimensions. We will see exactly what this means in-depth later. First,
lets look what we can do with tensors.



.. code-block:: python

    # Author: Robert Guthrie

    import torch
    import torch.autograd as autograd
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim

    torch.manual_seed(1)








Creating Tensors
~~~~~~~~~~~~~~~~

Tensors can be created from Python lists with the torch.Tensor()
function.




.. code-block:: python


    # Create a torch.Tensor object with the given data.  It is a 1D vector
    V_data = [1., 2., 3.]
    V = torch.Tensor(V_data)
    print(V)

    # Creates a matrix
    M_data = [[1., 2., 3.], [4., 5., 6]]
    M = torch.Tensor(M_data)
    print(M)

    # Create a 3D tensor of size 2x2x2.
    T_data = [[[1., 2.], [3., 4.]],
              [[5., 6.], [7., 8.]]]
    T = torch.Tensor(T_data)
    print(T)






.. rst-class:: sphx-glr-script-out

 Out::

    1
     2
     3
    [torch.FloatTensor of size 3]


     1  2  3
     4  5  6
    [torch.FloatTensor of size 2x3]


    (0 ,.,.) = 
      1  2
      3  4

    (1 ,.,.) = 
      5  6
      7  8
    [torch.FloatTensor of size 2x2x2]


What is a 3D tensor anyway? Think about it like this. If you have a
vector, indexing into the vector gives you a scalar. If you have a
matrix, indexing into the matrix gives you a vector. If you have a 3D
tensor, then indexing into the tensor gives you a matrix!

A note on terminology:
when I say "tensor" in this tutorial, it refers
to any torch.Tensor object. Matrices and vectors are special cases of
torch.Tensors, where their dimension is 1 and 2 respectively. When I am
talking about 3D tensors, I will explicitly use the term "3D tensor".




.. code-block:: python


    # Index into V and get a scalar
    print(V[0])

    # Index into M and get a vector
    print(M[0])

    # Index into T and get a matrix
    print(T[0])






.. rst-class:: sphx-glr-script-out

 Out::

    1.0

     1
     2
     3
    [torch.FloatTensor of size 3]


     1  2
     3  4
    [torch.FloatTensor of size 2x2]


You can also create tensors of other datatypes. The default, as you can
see, is Float. To create a tensor of integer types, try
torch.LongTensor(). Check the documentation for more data types, but
Float and Long will be the most common.



You can create a tensor with random data and the supplied dimensionality
with torch.randn()




.. code-block:: python


    x = torch.randn((3, 4, 5))
    print(x)






.. rst-class:: sphx-glr-script-out

 Out::

    (0 ,.,.) = 
     -2.9718  1.7070 -0.4305 -2.2820  0.5237
      0.0004 -1.2039  3.5283  0.4434  0.5848
      0.8407  0.5510  0.3863  0.9124 -0.8410
      1.2282 -1.8661  1.4146 -1.8781 -0.4674

    (1 ,.,.) = 
     -0.7576  0.4215 -0.4827 -1.1198  0.3056
      1.0386  0.5206 -0.5006  1.2182  0.2117
     -1.0613 -1.9441 -0.9596  0.5489 -0.9901
     -0.3826  1.5037  1.8267  0.5561  1.6445

    (2 ,.,.) = 
      0.4973 -1.5067  1.7661 -0.3569 -0.1713
      0.4068 -0.4284 -1.1299  1.4274 -1.4027
      1.4825 -1.1559  1.6190  0.9581  0.7747
      0.1940  0.1687  0.3061  1.0743 -1.0327
    [torch.FloatTensor of size 3x4x5]


Operations with Tensors
~~~~~~~~~~~~~~~~~~~~~~~

You can operate on tensors in the ways you would expect.



.. code-block:: python


    x = torch.Tensor([1., 2., 3.])
    y = torch.Tensor([4., 5., 6.])
    z = x + y
    print(z)






.. rst-class:: sphx-glr-script-out

 Out::

    5
     7
     9
    [torch.FloatTensor of size 3]


See `the documentation <http://pytorch.org/docs/torch.html>`__ for a
complete list of the massive number of operations available to you. They
expand beyond just mathematical operations.

One helpful operation that we will make use of later is concatenation.




.. code-block:: python


    # By default, it concatenates along the first axis (concatenates rows)
    x_1 = torch.randn(2, 5)
    y_1 = torch.randn(3, 5)
    z_1 = torch.cat([x_1, y_1])
    print(z_1)

    # Concatenate columns:
    x_2 = torch.randn(2, 3)
    y_2 = torch.randn(2, 5)
    # second arg specifies which axis to concat along
    z_2 = torch.cat([x_2, y_2], 1)
    print(z_2)

    # If your tensors are not compatible, torch will complain.  Uncomment to see the error
    # torch.cat([x_1, x_2])






.. rst-class:: sphx-glr-script-out

 Out::

    1.0930  0.7769 -1.3128  0.7099  0.9944
    -0.2694 -0.6491 -0.1373 -0.2954 -0.7725
    -0.2215  0.5074 -0.6794 -1.6115  0.5230
    -0.8890  0.2620  0.0302  0.0013 -1.3987
     1.4666 -0.1028 -0.0097 -0.8420 -0.2067
    [torch.FloatTensor of size 5x5]


     1.0672  0.1732 -0.6873  0.3620  0.3776 -0.2443 -0.5850  2.0812
     0.3111  0.2358 -1.0658 -0.1186  0.4903  0.8349  0.8894  0.4148
    [torch.FloatTensor of size 2x8]


Reshaping Tensors
~~~~~~~~~~~~~~~~~

Use the .view() method to reshape a tensor. This method receives heavy
use, because many neural network components expect their inputs to have
a certain shape. Often you will need to reshape before passing your data
to the component.




.. code-block:: python


    x = torch.randn(2, 3, 4)
    print(x)
    print(x.view(2, 12))  # Reshape to 2 rows, 12 columns
    # Same as above.  If one of the dimensions is -1, its size can be inferred
    print(x.view(2, -1))






.. rst-class:: sphx-glr-script-out

 Out::

    (0 ,.,.) = 
      0.0507 -0.9644 -2.0111  0.5245
      2.1332 -0.0822  0.8388 -1.3233
      0.0701  1.2200  0.4251 -1.2328

    (1 ,.,.) = 
     -0.6195  1.5133  1.9954 -0.6585
     -0.4139 -0.2250 -0.6890  0.9882
      0.7404 -2.0990  1.2582 -0.3990
    [torch.FloatTensor of size 2x3x4]



    Columns 0 to 9 
     0.0507 -0.9644 -2.0111  0.5245  2.1332 -0.0822  0.8388 -1.3233  0.0701  1.2200
    -0.6195  1.5133  1.9954 -0.6585 -0.4139 -0.2250 -0.6890  0.9882  0.7404 -2.0990

    Columns 10 to 11 
     0.4251 -1.2328
     1.2582 -0.3990
    [torch.FloatTensor of size 2x12]



    Columns 0 to 9 
     0.0507 -0.9644 -2.0111  0.5245  2.1332 -0.0822  0.8388 -1.3233  0.0701  1.2200
    -0.6195  1.5133  1.9954 -0.6585 -0.4139 -0.2250 -0.6890  0.9882  0.7404 -2.0990

    Columns 10 to 11 
     0.4251 -1.2328
     1.2582 -0.3990
    [torch.FloatTensor of size 2x12]


Computation Graphs and Automatic Differentiation
================================================

The concept of a computation graph is essential to efficient deep
learning programming, because it allows you to not have to write the
back propagation gradients yourself. A computation graph is simply a
specification of how your data is combined to give you the output. Since
the graph totally specifies what parameters were involved with which
operations, it contains enough information to compute derivatives. This
probably sounds vague, so lets see what is going on using the
fundamental class of Pytorch: autograd.Variable.

First, think from a programmers perspective. What is stored in the
torch.Tensor objects we were creating above? Obviously the data and the
shape, and maybe a few other things. But when we added two tensors
together, we got an output tensor. All this output tensor knows is its
data and shape. It has no idea that it was the sum of two other tensors
(it could have been read in from a file, it could be the result of some
other operation, etc.)

The Variable class keeps track of how it was created. Lets see it in
action.




.. code-block:: python


    # Variables wrap tensor objects
    x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True)
    # You can access the data with the .data attribute
    print(x.data)

    # You can also do all the same operations you did with tensors with Variables.
    y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True)
    z = x + y
    print(z.data)

    # BUT z knows something extra.
    print(z.grad_fn)






.. rst-class:: sphx-glr-script-out

 Out::

    1
     2
     3
    [torch.FloatTensor of size 3]


     5
     7
     9
    [torch.FloatTensor of size 3]

    <torch.autograd.function.AddBackward object at 0x7f85b3f34808>


So Variables know what created them. z knows that it wasn't read in from
a file, it wasn't the result of a multiplication or exponential or
whatever. And if you keep following z.grad_fn, you will find yourself at
x and y.

But how does that help us compute a gradient?




.. code-block:: python


    # Lets sum up all the entries in z
    s = z.sum()
    print(s)
    print(s.grad_fn)






.. rst-class:: sphx-glr-script-out

 Out::

    Variable containing:
     21
    [torch.FloatTensor of size 1]

    <torch.autograd.function.SumBackward object at 0x7f85b3f34be8>


So now, what is the derivative of this sum with respect to the first
component of x? In math, we want

.. math::

   \frac{\partial s}{\partial x_0}



Well, s knows that it was created as a sum of the tensor z. z knows
that it was the sum x + y. So

.. math::  s = \overbrace{x_0 + y_0}^\text{$z_0$} + \overbrace{x_1 + y_1}^\text{$z_1$} + \overbrace{x_2 + y_2}^\text{$z_2$}

And so s contains enough information to determine that the derivative
we want is 1!

Of course this glosses over the challenge of how to actually compute
that derivative. The point here is that s is carrying along enough
information that it is possible to compute it. In reality, the
developers of Pytorch program the sum() and + operations to know how to
compute their gradients, and run the back propagation algorithm. An
in-depth discussion of that algorithm is beyond the scope of this
tutorial.



Lets have Pytorch compute the gradient, and see that we were right:
(note if you run this block multiple times, the gradient will increment.
That is because Pytorch *accumulates* the gradient into the .grad
property, since for many models this is very convenient.)




.. code-block:: python


    # calling .backward() on any variable will run backprop, starting from it.
    s.backward()
    print(x.grad)






.. rst-class:: sphx-glr-script-out

 Out::

    Variable containing:
     1
     1
     1
    [torch.FloatTensor of size 3]


Understanding what is going on in the block below is crucial for being a
successful programmer in deep learning.




.. code-block:: python


    x = torch.randn((2, 2))
    y = torch.randn((2, 2))
    z = x + y  # These are Tensor types, and backprop would not be possible

    var_x = autograd.Variable(x)
    var_y = autograd.Variable(y)
    # var_z contains enough information to compute gradients, as we saw above
    var_z = var_x + var_y
    print(var_z.grad_fn)

    var_z_data = var_z.data  # Get the wrapped Tensor object out of var_z...
    # Re-wrap the tensor in a new variable
    new_var_z = autograd.Variable(var_z_data)

    # ... does new_var_z have information to backprop to x and y?
    # NO!
    print(new_var_z.grad_fn)
    # And how could it?  We yanked the tensor out of var_z (that is
    # what var_z.data is).  This tensor doesn't know anything about
    # how it was computed.  We pass it into new_var_z, and this is all the
    # information new_var_z gets.  If var_z_data doesn't know how it was
    # computed, theres no way new_var_z will.
    # In essence, we have broken the variable away from its past history






.. rst-class:: sphx-glr-script-out

 Out::

    <torch.autograd.function.AddBackward object at 0x7f85b3f34618>
    None


Here is the basic, extremely important rule for computing with
autograd.Variables (note this is more general than Pytorch. There is an
equivalent object in every major deep learning toolkit):

**If you want the error from your loss function to backpropogate to a
component of your network, you MUST NOT break the Variable chain from
that component to your loss Variable. If you do, the loss will have no
idea your component exists, and its parameters can't be updated.**

I say this in bold, because this error can creep up on you in very
subtle ways (I will show some such ways below), and it will not cause
your code to crash or complain, so you must be careful.



**Total running time of the script:** ( 0 minutes  0.006 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: pytorch_tutorial.py <pytorch_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: pytorch_tutorial.ipynb <pytorch_tutorial.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
