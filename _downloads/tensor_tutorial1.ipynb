{
  "metadata": {
    "language_info": {
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "version": "3.5.2",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py"
    },
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "\nTensors\n=======\n\nTensors behave almost exactly the same way in PyTorch as they do in\nTorch.\n\nCreate a tensor of size (5 x 7) with uninitialized memory:\n\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch\na = torch.FloatTensor(5, 7)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Initialize a tensor randomized with a normal distribution with mean=0, var=1:\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "a = torch.randn(5, 7)\nprint(a)\nprint(a.size())"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports the same operations</p></div>\n\nInplace / Out-of-place\n----------------------\n\nThe first difference is that ALL operations on the tensor that operate\nin-place on it will have an ``_`` postfix. For example, ``add`` is the\nout-of-place version, and ``add_`` is the in-place version.\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "a.fill_(3.5)\n# a has now been filled with the value 3.5\n\nb = a.add(4.0)\n# a is still filled with 3.5\n# new tensor b is returned with values 3.5 + 4.0 = 7.5\n\nprint(a, b)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Some operations like ``narrow`` do not have in-place versions, and\nhence, ``.narrow_`` does not exist. Similarly, some operations like\n``fill_`` do not have an out-of-place version, so ``.fill`` does not\nexist.\n\nZero Indexing\n-------------\n\nAnother difference is that Tensors are zero-indexed. (In lua, tensors are\none-indexed)\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "b = a[0, 3]  # select 1st row, 4th column from a"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Tensors can be also indexed with Python's slicing\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "No camel casing\n---------------\n\nThe next small difference is that all functions are now NOT camelCase\nanymore. For example ``indexAdd`` is now called ``index_add_``\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "x = torch.ones(5, 5)\nprint(x)"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "z = torch.Tensor(5, 2)\nz[:, 0] = 10\nz[:, 1] = 100\nprint(z)"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "x.index_add_(1, torch.LongTensor([4, 0]), z)\nprint(x)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Numpy Bridge\n------------\n\nConverting a torch Tensor to a numpy array and vice versa is a breeze.\nThe torch Tensor and numpy array will share their underlying memory\nlocations, and changing one will change the other.\n\nConverting torch Tensor to numpy Array\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "a = torch.ones(5)\nprint(a)"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "b = a.numpy()\nprint(b)"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "a.add_(1)\nprint(a)\nprint(b) \t# see how the numpy array changed in value"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Converting numpy Array to torch Tensor\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)  # see how changing the np array changed the torch Tensor automatically"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "All the Tensors on the CPU except a CharTensor support converting to\nNumPy and back.\n\nCUDA Tensors\n------------\n\nCUDA Tensors are nice and easy in pytorch, and they are much more\nconsistent as well. Transfering a CUDA tensor from the CPU to GPU will\nretain it\u2019s type.\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# let us run this cell only if CUDA is available\nif torch.cuda.is_available():\n    # creates a LongTensor and transfers it\n    # to GPU as torch.cuda.LongTensor\n    a = torch.LongTensor(10).fill_(3).cuda()\n    print(type(a))\n    b = a.cpu()\n    # transfers it to CPU, back to\n    # being a torch.LongTensor"
      ]
    }
  ]
}