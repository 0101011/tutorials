.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_beginner_audio_classifier_tutorial.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_audio_classifier_tutorial.py:


Audio Classifier Tutorial
=========================
**Author**: `Winston Herring <https://github.com/winston6>`_

This tutorial will show you how to correctly format an audio dataset and
then train/test an audio classifier network on the dataset. First, let’s
import the common torch packages as well as ``torchaudio``, ``pandas``,
and ``numpy``. ``torchaudio`` is available `here <https://github.com/pytorch/audio>`_
and can be installed by following the
instructions on the website.




.. code-block:: python


    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torchvision import datasets, transforms
    from torch.utils.data import Dataset
    import torchaudio
    import pandas as pd
    import numpy as np








Let’s check if a CUDA GPU is available and select our device. Running
the network on a GPU will greatly decrease the training/testing runtime.




.. code-block:: python


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    cuda


Importing the Dataset
---------------------

We will use the UrbanSound8K dataset to train our network. It is
available for free `here <https://urbansounddataset.weebly.com/>`_ and contains
10 audio classes with over 8000 audio samples! Once you have downloaded
the compressed dataset, extract it to your current working directory.
First, we will look at the csv file that provides information about the
individual sound files. ``pandas`` allows us to open the csv file and
use ``.iloc()`` to access the data within it.




.. code-block:: python


    csvData = pd.read_csv('./data/UrbanSound8K/metadata/UrbanSound8K.csv')
    print(csvData.iloc[0, :])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    slice_file_name    100032-3-0-0.wav
    fsID                         100032
    start                             0
    end                        0.317551
    salience                          1
    fold                              5
    classID                           3
    class                      dog_bark
    Name: 0, dtype: object


The 10 audio classes in the UrbanSound8K dataset are air_conditioner,
car_horn, children_playing, dog_bark, drilling, enginge_idling,
gun_shot, jackhammer, siren, and street_music. Let’s play a couple files
and see what they sound like. The first file is street music and the
second is an air conditioner.




.. code-block:: python


    import IPython.display as ipd
    ipd.Audio('./data/UrbanSound8K/audio/fold1/108041-9-0-5.wav')

    ipd.Audio('./data/UrbanSound8K/audio/fold5/100852-0-0-19.wav')








Formatting the Data
-------------------

Now that we know the format of the csv file entries, we can construct
our dataset. We will create a rapper class for our dataset using
``torch.utils.data.Dataset`` that will handle loading the files and
performing some formatting steps. The UrbanSound8K dataset is separated
into 10 folders. We will use the data from 9 of these folders to train
our network and then use the 10th folder to test the network. The rapper
class will store the file names, labels, and folder numbers of the audio
files in the inputted folder list when initialized. The actual loading
and formatting steps will happen in the access function ``__getitem__``.

In ``__getitem__``, we use ``torchaudio.load()`` to convert the wav
files to tensors. ``torchaudio.load()`` returns a tuple containing the
newly created tensor along with the sampling frequency of the audio file
(44.1kHz for UrbanSound8K). The dataset uses two channels for audio so
we will use ``torchaudio.transforms.DownmixMono()`` to convert the audio
data to one channel. Next, we need to format the audio data. The network
we will make takes an input size of 32,000, while most of the audio
files have well over 100,000 samples. The UrbanSound8K audio is sampled
at 44.1kHz, so 32,000 samples only covers around 700 milliseconds. By
downsampling the audio to aproximately 8kHz, we can represent 4 seconds
with the 32,000 samples. This downsampling is achieved by taking every
fifth sample of the original audio tensor. Not every audio tensor is
long enough to handle the downsampling so these tensors will need to be
padded with zeros. The minimum length that won’t require padding is
160,000 samples.




.. code-block:: python


    class UrbanSoundDataset(Dataset):
    #rapper for the UrbanSound8K dataset
        # Argument List
        #  path to the UrbanSound8K csv file
        #  path to the UrbanSound8K audio files
        #  list of folders to use in the dataset
    
        def __init__(self, csv_path, file_path, folderList):
            csvData = pd.read_csv(csv_path)
            #initialize lists to hold file names, labels, and folder numbers
            self.file_names = []
            self.labels = []
            self.folders = []
            #loop through the csv entries and only add entries from folders in the folder list
            for i in range(0,len(csvData)):
                if csvData.iloc[i, 5] in folderList:
                    self.file_names.append(csvData.iloc[i, 0])
                    self.labels.append(csvData.iloc[i, 6])
                    self.folders.append(csvData.iloc[i, 5])
                
            self.file_path = file_path
            self.mixer = torchaudio.transforms.DownmixMono() #UrbanSound8K uses two channels, this will convert them to one
            self.folderList = folderList
        
        def __getitem__(self, index):
            #format the file path and load the file
            path = self.file_path + "fold" + str(self.folders[index]) + "/" + self.file_names[index]
            sound = torchaudio.load(path, out = None, normalization = True)
            #load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)
            soundData = self.mixer(sound[0])
            #downsample the audio to ~8kHz
            tempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short
            if soundData.numel() < 160000:
                tempData[:soundData.numel()] = soundData[:]
            else:
                tempData[:] = soundData[:160000]
        
            soundData = tempData
            soundFormatted = torch.zeros([32000, 1])
            soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData
            soundFormatted = soundFormatted.permute(1, 0)
            return soundFormatted, self.labels[index]
    
        def __len__(self):
            return len(self.file_names)

    
    csv_path = './data/UrbanSound8K/metadata/UrbanSound8K.csv'
    file_path = './data/UrbanSound8K/audio/'

    train_set = UrbanSoundDataset(csv_path, file_path, range(1,10))
    test_set = UrbanSoundDataset(csv_path, file_path, [10])
    print("Train set size: " + str(len(train_set)))
    print("Test set size: " + str(len(test_set)))

    kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu

    train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Train set size: 7895
    Test set size: 837


Define the Network
------------------

For this tutorial we will use a convolutional neural network to process
the raw audio data. Usually more advanced transforms are applied to the
audio data, however CNNs can be used to accurately process the raw data.
The specific architecture is modeled after the M5 network architecture
described in https://arxiv.org/pdf/1610.00087.pdf. An important aspect
of models processing raw audio data is the receptive field of their
first layer’s filters. Our model’s first filter is length 80 so when
processing audio sampled at 8kHz the receptive field is around 10ms.
This size is similar to speech processing applications that often use
receptive fields ranging from 20ms to 40ms.




.. code-block:: python


    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv1d(1, 128, 80, 4)
            self.bn1 = nn.BatchNorm1d(128)
            self.pool1 = nn.MaxPool1d(4)
            self.conv2 = nn.Conv1d(128, 128, 3)
            self.bn2 = nn.BatchNorm1d(128)
            self.pool2 = nn.MaxPool1d(4)
            self.conv3 = nn.Conv1d(128, 256, 3)
            self.bn3 = nn.BatchNorm1d(256)
            self.pool3 = nn.MaxPool1d(4)
            self.conv4 = nn.Conv1d(256, 512, 3)
            self.bn4 = nn.BatchNorm1d(512)
            self.pool4 = nn.MaxPool1d(4)
            self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1
            self.fc1 = nn.Linear(512, 10)
        
        def forward(self, x):
            x = self.conv1(x)
            x = F.relu(self.bn1(x))
            x = self.pool1(x)
            x = self.conv2(x)
            x = F.relu(self.bn2(x))
            x = self.pool2(x)
            x = self.conv3(x)
            x = F.relu(self.bn3(x))
            x = self.pool3(x)
            x = self.conv4(x)
            x = F.relu(self.bn4(x))
            x = self.pool4(x)
            x = self.avgPool(x)
            x = x.permute(0, 2, 1) #change the 512x1 to 1x512
            x = self.fc1(x)
            return F.log_softmax(x, dim = 2)

    model = Net()
    model.to(device)
    print(model)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Net(
      (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))
      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))
      (fc1): Linear(in_features=512, out_features=10, bias=True)
    )


We will use the same optimization technique used in the paper, an Adam
optimizer with weight decay set to 0.0001. At first, we will train with
a learning rate of 0.01, but we will use a ``scheduler`` to decrease it
to 0.001 during training.




.. code-block:: python


    optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)








Training and Testing the Network
--------------------------------

Now let’s define a training function that will feed our training data
into the model and perform the backward pass and optimization steps.




.. code-block:: python


    def train(model, epoch):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            data = data.to(device)
            target = target.to(device)
            data = data.requires_grad_() #set requires_grad to True for training
            output = model(data)
            output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 
            loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input
            loss.backward()
            optimizer.step()
            if batch_idx % log_interval == 0: #print training stats
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    100. * batch_idx / len(train_loader), loss))








Now that we have a training function, we need to make one for testing
the networks accuracy. We will set the model to ``eval()`` mode and then
run inference on the test dataset. Calling ``eval()`` sets the training
variable in all modules in the network to false. Certain layers like
batch normalization and dropout layers behave differently during
training so this step is crucial for getting correct results.




.. code-block:: python


    def test(model, epoch):
        model.eval()
        correct = 0
        for data, target in test_loader:
            data = data.to(device)
            target = target.to(device)
            output = model(data)
            output = output.permute(1, 0, 2)
            pred = output.max(2)[1] # get the index of the max log-probability
            correct += pred.eq(target).cpu().sum().item()
        print('\nTest set: Accuracy: {}/{} ({:.0f}%)\n'.format(
            correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset)))








Finally, we can train and test the network. We will train the network
for ten epochs then reduce the learn rate and train for ten more epochs.
The network will be tested after each epoch to see how the accuracy
varies during the training.




.. code-block:: python


    log_interval = 20
    for epoch in range(1, 41):
        if epoch == 31:
            print("First round of training complete. Setting learn rate to 0.001.")
        scheduler.step()
        train(model, epoch)
        test(model, epoch)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Train Epoch: 1 [0/7895 (0%)]    Loss: 2.401783
    Train Epoch: 1 [2560/7895 (32%)]        Loss: 1.942908
    Train Epoch: 1 [5120/7895 (65%)]        Loss: 1.611913
    Train Epoch: 1 [7680/7895 (97%)]        Loss: 1.537676

    Test set: Accuracy: 263/837 (31%)

    Train Epoch: 2 [0/7895 (0%)]    Loss: 1.448736
    Train Epoch: 2 [2560/7895 (32%)]        Loss: 1.386669
    Train Epoch: 2 [5120/7895 (65%)]        Loss: 1.701678
    Train Epoch: 2 [7680/7895 (97%)]        Loss: 1.308515

    Test set: Accuracy: 319/837 (38%)

    Train Epoch: 3 [0/7895 (0%)]    Loss: 1.390272
    Train Epoch: 3 [2560/7895 (32%)]        Loss: 1.566906
    Train Epoch: 3 [5120/7895 (65%)]        Loss: 1.182938
    Train Epoch: 3 [7680/7895 (97%)]        Loss: 1.179093

    Test set: Accuracy: 429/837 (51%)

    Train Epoch: 4 [0/7895 (0%)]    Loss: 1.189586
    Train Epoch: 4 [2560/7895 (32%)]        Loss: 1.143465
    Train Epoch: 4 [5120/7895 (65%)]        Loss: 1.267664
    Train Epoch: 4 [7680/7895 (97%)]        Loss: 1.240582

    Test set: Accuracy: 305/837 (36%)

    Train Epoch: 5 [0/7895 (0%)]    Loss: 1.258480
    Train Epoch: 5 [2560/7895 (32%)]        Loss: 1.108861
    Train Epoch: 5 [5120/7895 (65%)]        Loss: 1.062089
    Train Epoch: 5 [7680/7895 (97%)]        Loss: 1.045210

    Test set: Accuracy: 442/837 (53%)

    Train Epoch: 6 [0/7895 (0%)]    Loss: 0.845483
    Train Epoch: 6 [2560/7895 (32%)]        Loss: 1.074444
    Train Epoch: 6 [5120/7895 (65%)]        Loss: 0.959148
    Train Epoch: 6 [7680/7895 (97%)]        Loss: 1.194646

    Test set: Accuracy: 470/837 (56%)

    Train Epoch: 7 [0/7895 (0%)]    Loss: 1.032602
    Train Epoch: 7 [2560/7895 (32%)]        Loss: 0.953928
    Train Epoch: 7 [5120/7895 (65%)]        Loss: 1.062960
    Train Epoch: 7 [7680/7895 (97%)]        Loss: 1.228553

    Test set: Accuracy: 446/837 (53%)

    Train Epoch: 8 [0/7895 (0%)]    Loss: 0.892770
    Train Epoch: 8 [2560/7895 (32%)]        Loss: 1.012779
    Train Epoch: 8 [5120/7895 (65%)]        Loss: 0.925422
    Train Epoch: 8 [7680/7895 (97%)]        Loss: 0.887139

    Test set: Accuracy: 367/837 (44%)

    Train Epoch: 9 [0/7895 (0%)]    Loss: 0.839052
    Train Epoch: 9 [2560/7895 (32%)]        Loss: 0.903886
    Train Epoch: 9 [5120/7895 (65%)]        Loss: 0.880377
    Train Epoch: 9 [7680/7895 (97%)]        Loss: 0.842459

    Test set: Accuracy: 402/837 (48%)

    Train Epoch: 10 [0/7895 (0%)]   Loss: 0.872950
    Train Epoch: 10 [2560/7895 (32%)]       Loss: 0.881906
    Train Epoch: 10 [5120/7895 (65%)]       Loss: 0.900106
    Train Epoch: 10 [7680/7895 (97%)]       Loss: 0.818573

    Test set: Accuracy: 462/837 (55%)

    Train Epoch: 11 [0/7895 (0%)]   Loss: 0.775962
    Train Epoch: 11 [2560/7895 (32%)]       Loss: 0.764272
    Train Epoch: 11 [5120/7895 (65%)]       Loss: 0.841936
    Train Epoch: 11 [7680/7895 (97%)]       Loss: 0.775175

    Test set: Accuracy: 399/837 (48%)

    Train Epoch: 12 [0/7895 (0%)]   Loss: 0.789093
    Train Epoch: 12 [2560/7895 (32%)]       Loss: 0.752719
    Train Epoch: 12 [5120/7895 (65%)]       Loss: 0.743893
    Train Epoch: 12 [7680/7895 (97%)]       Loss: 0.688283

    Test set: Accuracy: 437/837 (52%)

    Train Epoch: 13 [0/7895 (0%)]   Loss: 0.634692
    Train Epoch: 13 [2560/7895 (32%)]       Loss: 0.672826
    Train Epoch: 13 [5120/7895 (65%)]       Loss: 0.875691
    Train Epoch: 13 [7680/7895 (97%)]       Loss: 0.680302

    Test set: Accuracy: 481/837 (57%)

    Train Epoch: 14 [0/7895 (0%)]   Loss: 0.803486
    Train Epoch: 14 [2560/7895 (32%)]       Loss: 0.681769
    Train Epoch: 14 [5120/7895 (65%)]       Loss: 0.540109
    Train Epoch: 14 [7680/7895 (97%)]       Loss: 0.784313

    Test set: Accuracy: 469/837 (56%)

    Train Epoch: 15 [0/7895 (0%)]   Loss: 0.848115
    Train Epoch: 15 [2560/7895 (32%)]       Loss: 0.406294
    Train Epoch: 15 [5120/7895 (65%)]       Loss: 0.642965
    Train Epoch: 15 [7680/7895 (97%)]       Loss: 0.597260

    Test set: Accuracy: 388/837 (46%)

    Train Epoch: 16 [0/7895 (0%)]   Loss: 0.470694
    Train Epoch: 16 [2560/7895 (32%)]       Loss: 0.553741
    Train Epoch: 16 [5120/7895 (65%)]       Loss: 0.611447
    Train Epoch: 16 [7680/7895 (97%)]       Loss: 0.787738

    Test set: Accuracy: 441/837 (53%)

    Train Epoch: 17 [0/7895 (0%)]   Loss: 0.544644
    Train Epoch: 17 [2560/7895 (32%)]       Loss: 0.428352
    Train Epoch: 17 [5120/7895 (65%)]       Loss: 0.656771
    Train Epoch: 17 [7680/7895 (97%)]       Loss: 0.573583

    Test set: Accuracy: 509/837 (61%)

    Train Epoch: 18 [0/7895 (0%)]   Loss: 0.660233
    Train Epoch: 18 [2560/7895 (32%)]       Loss: 0.469369
    Train Epoch: 18 [5120/7895 (65%)]       Loss: 0.683348
    Train Epoch: 18 [7680/7895 (97%)]       Loss: 0.533530

    Test set: Accuracy: 416/837 (50%)

    Train Epoch: 19 [0/7895 (0%)]   Loss: 0.660491
    Train Epoch: 19 [2560/7895 (32%)]       Loss: 0.701841
    Train Epoch: 19 [5120/7895 (65%)]       Loss: 0.632332
    Train Epoch: 19 [7680/7895 (97%)]       Loss: 0.704002

    Test set: Accuracy: 462/837 (55%)

    Train Epoch: 20 [0/7895 (0%)]   Loss: 0.524171
    Train Epoch: 20 [2560/7895 (32%)]       Loss: 0.387633
    Train Epoch: 20 [5120/7895 (65%)]       Loss: 0.495510
    Train Epoch: 20 [7680/7895 (97%)]       Loss: 0.511025

    Test set: Accuracy: 434/837 (52%)

    Train Epoch: 21 [0/7895 (0%)]   Loss: 0.635646
    Train Epoch: 21 [2560/7895 (32%)]       Loss: 0.574998
    Train Epoch: 21 [5120/7895 (65%)]       Loss: 0.462211
    Train Epoch: 21 [7680/7895 (97%)]       Loss: 0.338539

    Test set: Accuracy: 517/837 (62%)

    Train Epoch: 22 [0/7895 (0%)]   Loss: 0.438384
    Train Epoch: 22 [2560/7895 (32%)]       Loss: 0.390657
    Train Epoch: 22 [5120/7895 (65%)]       Loss: 0.455439
    Train Epoch: 22 [7680/7895 (97%)]       Loss: 0.456629

    Test set: Accuracy: 530/837 (63%)

    Train Epoch: 23 [0/7895 (0%)]   Loss: 0.390882
    Train Epoch: 23 [2560/7895 (32%)]       Loss: 0.382214
    Train Epoch: 23 [5120/7895 (65%)]       Loss: 0.387333
    Train Epoch: 23 [7680/7895 (97%)]       Loss: 0.278331

    Test set: Accuracy: 543/837 (65%)

    Train Epoch: 24 [0/7895 (0%)]   Loss: 0.349626
    Train Epoch: 24 [2560/7895 (32%)]       Loss: 0.394519
    Train Epoch: 24 [5120/7895 (65%)]       Loss: 0.303064
    Train Epoch: 24 [7680/7895 (97%)]       Loss: 0.349828

    Test set: Accuracy: 526/837 (63%)

    Train Epoch: 25 [0/7895 (0%)]   Loss: 0.280841
    Train Epoch: 25 [2560/7895 (32%)]       Loss: 0.324507
    Train Epoch: 25 [5120/7895 (65%)]       Loss: 0.429600
    Train Epoch: 25 [7680/7895 (97%)]       Loss: 0.247160

    Test set: Accuracy: 548/837 (65%)

    Train Epoch: 26 [0/7895 (0%)]   Loss: 0.251200
    Train Epoch: 26 [2560/7895 (32%)]       Loss: 0.404395
    Train Epoch: 26 [5120/7895 (65%)]       Loss: 0.350482
    Train Epoch: 26 [7680/7895 (97%)]       Loss: 0.364785

    Test set: Accuracy: 525/837 (63%)

    Train Epoch: 27 [0/7895 (0%)]   Loss: 0.316140
    Train Epoch: 27 [2560/7895 (32%)]       Loss: 0.354951
    Train Epoch: 27 [5120/7895 (65%)]       Loss: 0.324946
    Train Epoch: 27 [7680/7895 (97%)]       Loss: 0.284883

    Test set: Accuracy: 532/837 (64%)

    Train Epoch: 28 [0/7895 (0%)]   Loss: 0.291878
    Train Epoch: 28 [2560/7895 (32%)]       Loss: 0.235400
    Train Epoch: 28 [5120/7895 (65%)]       Loss: 0.219001
    Train Epoch: 28 [7680/7895 (97%)]       Loss: 0.384807

    Test set: Accuracy: 533/837 (64%)

    Train Epoch: 29 [0/7895 (0%)]   Loss: 0.233841
    Train Epoch: 29 [2560/7895 (32%)]       Loss: 0.280808
    Train Epoch: 29 [5120/7895 (65%)]       Loss: 0.288520
    Train Epoch: 29 [7680/7895 (97%)]       Loss: 0.276871

    Test set: Accuracy: 522/837 (62%)

    Train Epoch: 30 [0/7895 (0%)]   Loss: 0.267946
    Train Epoch: 30 [2560/7895 (32%)]       Loss: 0.282954
    Train Epoch: 30 [5120/7895 (65%)]       Loss: 0.387572
    Train Epoch: 30 [7680/7895 (97%)]       Loss: 0.329959

    Test set: Accuracy: 525/837 (63%)

    First round of training complete. Setting learn rate to 0.001.
    Train Epoch: 31 [0/7895 (0%)]   Loss: 0.263517
    Train Epoch: 31 [2560/7895 (32%)]       Loss: 0.274979
    Train Epoch: 31 [5120/7895 (65%)]       Loss: 0.320370
    Train Epoch: 31 [7680/7895 (97%)]       Loss: 0.328631

    Test set: Accuracy: 541/837 (65%)

    Train Epoch: 32 [0/7895 (0%)]   Loss: 0.279226
    Train Epoch: 32 [2560/7895 (32%)]       Loss: 0.378607
    Train Epoch: 32 [5120/7895 (65%)]       Loss: 0.288329
    Train Epoch: 32 [7680/7895 (97%)]       Loss: 0.252895

    Test set: Accuracy: 541/837 (65%)

    Train Epoch: 33 [0/7895 (0%)]   Loss: 0.177180
    Train Epoch: 33 [2560/7895 (32%)]       Loss: 0.318159
    Train Epoch: 33 [5120/7895 (65%)]       Loss: 0.351586
    Train Epoch: 33 [7680/7895 (97%)]       Loss: 0.317728

    Test set: Accuracy: 539/837 (64%)

    Train Epoch: 34 [0/7895 (0%)]   Loss: 0.266558
    Train Epoch: 34 [2560/7895 (32%)]       Loss: 0.248662
    Train Epoch: 34 [5120/7895 (65%)]       Loss: 0.269173
    Train Epoch: 34 [7680/7895 (97%)]       Loss: 0.258319

    Test set: Accuracy: 551/837 (66%)

    Train Epoch: 35 [0/7895 (0%)]   Loss: 0.281909
    Train Epoch: 35 [2560/7895 (32%)]       Loss: 0.448420
    Train Epoch: 35 [5120/7895 (65%)]       Loss: 0.281279
    Train Epoch: 35 [7680/7895 (97%)]       Loss: 0.210610

    Test set: Accuracy: 536/837 (64%)

    Train Epoch: 36 [0/7895 (0%)]   Loss: 0.290286
    Train Epoch: 36 [2560/7895 (32%)]       Loss: 0.204914
    Train Epoch: 36 [5120/7895 (65%)]       Loss: 0.245726
    Train Epoch: 36 [7680/7895 (97%)]       Loss: 0.432974

    Test set: Accuracy: 533/837 (64%)

    Train Epoch: 37 [0/7895 (0%)]   Loss: 0.221967
    Train Epoch: 37 [2560/7895 (32%)]       Loss: 0.241110
    Train Epoch: 37 [5120/7895 (65%)]       Loss: 0.263667
    Train Epoch: 37 [7680/7895 (97%)]       Loss: 0.267553

    Test set: Accuracy: 533/837 (64%)

    Train Epoch: 38 [0/7895 (0%)]   Loss: 0.232275
    Train Epoch: 38 [2560/7895 (32%)]       Loss: 0.238140
    Train Epoch: 38 [5120/7895 (65%)]       Loss: 0.234631
    Train Epoch: 38 [7680/7895 (97%)]       Loss: 0.327993

    Test set: Accuracy: 531/837 (63%)

    Train Epoch: 39 [0/7895 (0%)]   Loss: 0.283075
    Train Epoch: 39 [2560/7895 (32%)]       Loss: 0.251312
    Train Epoch: 39 [5120/7895 (65%)]       Loss: 0.250645
    Train Epoch: 39 [7680/7895 (97%)]       Loss: 0.245870

    Test set: Accuracy: 549/837 (66%)

    Train Epoch: 40 [0/7895 (0%)]   Loss: 0.167454
    Train Epoch: 40 [2560/7895 (32%)]       Loss: 0.228703
    Train Epoch: 40 [5120/7895 (65%)]       Loss: 0.193202
    Train Epoch: 40 [7680/7895 (97%)]       Loss: 0.260973

    Test set: Accuracy: 560/837 (67%)


Conclusion
----------

If trained on 9 folders, the network should be more than 50% accurate by
the end of the training process. Training on less folders will result in
a lower overall accuracy but may be necessary if long runtimes are a
problem. Greater accuracies can be achieved using deeper CNNs at the
expense of a larger memory footprint.

For more advanced audio applications, such as speech recognition,
recurrent neural networks (RNNs) are commonly used. There are also other
data preprocessing methods, such as finding the mel frequency cepstral
coefficients (MFCC), that can reduce the size of the dataset.



**Total running time of the script:** ( 13 minutes  30.471 seconds)


.. _sphx_glr_download_beginner_audio_classifier_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: audio_classifier_tutorial.py <audio_classifier_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: audio_classifier_tutorial.ipynb <audio_classifier_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
